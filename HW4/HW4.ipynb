{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5tprgdro7hD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "5e02a00c-beaf-4fe6-d3bc-b7b1fcbbd29b"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import random\n",
        "import numpy as np\n",
        "!git clone https://github.com/danibachar/ImageUnderstandingUsingDeepLearningIDC2019.git\n",
        "%cd ImageUnderstandingUsingDeepLearningIDC2019/HW4/\n",
        "!pip uninstall scipy -y\n",
        "!pip install scipy==1.2.0\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "import matplotlib.pyplot as plt\n",
        "from data.img_utils import SQUEEZENET_MEAN, SQUEEZENET_STD\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ImageUnderstandingUsingDeepLearningIDC2019'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 4769 (delta 30), reused 66 (delta 16), pack-reused 4683\u001b[K\n",
            "Receiving objects: 100% (4769/4769), 338.33 MiB | 37.88 MiB/s, done.\n",
            "Resolving deltas: 100% (3781/3781), done.\n",
            "Checking out files: 100% (4553/4553), done.\n",
            "/content/ImageUnderstandingUsingDeepLearningIDC2019/HW4\n",
            "Uninstalling scipy-1.3.0:\n",
            "  Successfully uninstalled scipy-1.3.0\n",
            "Collecting scipy==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/e6/6d4edaceee6a110ecf6f318482f5229792f143e468b34a631f5a0899f56d/scipy-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (26.6MB)\n",
            "\u001b[K     |████████████████████████████████| 26.6MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.0) (1.16.4)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "Successfully installed scipy-1.2.0\n",
            "1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPoqFE7uo7hI",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Network Visualization (40 points)\n",
        "\n",
        "1. **Saliency Maps**: Saliency maps are a quick way to tell which part of the image influenced the classification decision made by the network.\n",
        "2. **Fooling Images**: We can perturb an input image so that it appears the same to humans, but will be misclassified by the pretrained network.\n",
        "3. **Class Visualization**: We can synthesize an image to maximize the classification score of a particular class; this can give us some sense of what the network is looking for when it classifies images of that class.\n",
        "\n",
        "\n",
        "## Helper Functions\n",
        "\n",
        "Our pretrained model was trained on images that had been preprocessed by subtracting the per-color mean and dividing by the per-color standard deviation. We define a few helper functions for performing and undoing this preprocessing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4GTfvz_o7hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(img, size=224):\n",
        "    transform = T.Compose([\n",
        "        T.Resize(size),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
        "                    std=SQUEEZENET_STD.tolist()),\n",
        "        T.Lambda(lambda x: x[None]),\n",
        "    ])\n",
        "    return transform(img)\n",
        "\n",
        "def deprocess(img, should_rescale=True):\n",
        "    transform = T.Compose([\n",
        "        T.Lambda(lambda x: x[0]),\n",
        "        T.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),\n",
        "        T.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),\n",
        "        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n",
        "        T.ToPILImage(),\n",
        "    ])\n",
        "    return transform(img)\n",
        "\n",
        "def rescale(x):\n",
        "    low, high = x.min(), x.max()\n",
        "    x_rescaled = (x - low) / (high - low)\n",
        "    return x_rescaled\n",
        "    \n",
        "def blur_image(X, sigma=1):\n",
        "    X_np = X.cpu().clone().numpy()\n",
        "    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n",
        "    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n",
        "    X.copy_(torch.Tensor(X_np).type_as(X))\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fignX-POo7hO",
        "colab_type": "text"
      },
      "source": [
        "## Pretrained model\n",
        "\n",
        "For all of our image generation experiments, we will start with a convolutional neural network which was pretrained to perform image classification on ImageNet. We can use any model here, but for the purposes of this assignment we will use [SqueezeNet](https://arxiv.org/abs/1602.07360), which achieves accuracies comparable to AlexNet but with a significantly reduced parameter count and computational complexity.\n",
        "\n",
        "Using SqueezeNet rather than AlexNet, VGG or ResNet means that we can easily perform all image generation experiments on CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mJXOjWBo7hO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and load the pretrained SqueezeNet model.\n",
        "model = torchvision.models.squeezenet1_1(pretrained=True)\n",
        "\n",
        "# We don't want to train the model, so tell PyTorch not to compute gradients\n",
        "# with respect to model parameters.\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0YhZCPOo7hR",
        "colab_type": "text"
      },
      "source": [
        "## Load some ImageNet images\n",
        "\n",
        "We have provided a few example images from the validation set of the ImageNet ILSVRC 2012 Classification dataset. Since they come from the validation set, our pretrained model did not see these images during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au2DZgoro7hS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data.data_utils import load_imagenet_val\n",
        "X, y, class_names = load_imagenet_val(num=5)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(5):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(X[i])\n",
        "    plt.title(class_names[y[i]])\n",
        "    plt.axis('off')\n",
        "plt.gcf().tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yLz_q_Lo7hU",
        "colab_type": "text"
      },
      "source": [
        "## Saliency Maps (20 points)\n",
        "Using this pretrained model, we will compute class saliency maps.\n",
        "\n",
        "A saliency map tells us the degree to which each pixel in the image affects the classification score for that image. To compute it, we compute the gradient of the unnormalized score corresponding to the correct class (which is a scalar) with respect to the pixels of the image. If the image has shape (3, H, W) then this gradient will also have shape (3, H, W); for each pixel in the image, this gradient tells us the amount by which the classification score will change if the pixel changes by a small amount. To compute the saliency map, we take the absolute value of this gradient, then take the maximum value over the 3 input channels; the final saliency map thus has shape (H, W) and all entries are nonnegative.\n",
        "\n",
        "### Hint: PyTorch gather method\n",
        "If `s` is an numpy array of shape `(N, C)` and `y` is a numpy array of shape `(N,)` containing integers `0 <= y[i] < C`, then `s[np.arange(N), y]` is a numpy array of shape `(N,)` which selects one element from each element in `s` using the indices in `y`.\n",
        "\n",
        "In PyTorch you can perform the same operation using the `gather()` method. If `s` is a PyTorch Tensor of shape `(N, C)` and `y` is a PyTorch Tensor of shape `(N,)` containing longs in the range `0 <= y[i] < C`, then `s.gather(1, y.view(-1, 1)).squeeze()` will be a PyTorch Tensor of shape `(N,)` containing one entry from each row of `s`, selected according to the indices in `y`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBjSC09Ko7hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gather_example():\n",
        "    N, C = 4, 5\n",
        "    s = torch.randn(N, C)\n",
        "    y = torch.LongTensor([1, 2, 1, 3])\n",
        "    print(s)\n",
        "    print(y)\n",
        "    print(s.gather(1, y.view(-1, 1)).squeeze())\n",
        "gather_example()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDSB1eQ5o7hZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_saliency_maps(X, y, model):\n",
        "    \"\"\"\n",
        "    Compute a class saliency map using the model for images X and labels y.\n",
        "\n",
        "    Input:\n",
        "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
        "    - y: Labels for X; LongTensor of shape (N,)\n",
        "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
        "\n",
        "    Returns:\n",
        "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
        "    images.\n",
        "    \"\"\"\n",
        "    saliency = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement this function. Perform a forward and backward pass through #\n",
        "    # the model to compute the gradient of the correct class score with respect  #\n",
        "    # to each input image. You first want to compute the loss over the correct   #\n",
        "    # scores (we'll combine losses across a batch by summing), and then compute  #\n",
        "    # the gradients with a backward pass.                                        #\n",
        "    ##############################################################################\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                             END OF YOUR CODE                               #\n",
        "    ##############################################################################\n",
        "    return saliency"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibdIjRqeo7hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_saliency_maps(X, y):\n",
        "    # Convert X and y from numpy arrays to Torch Tensors\n",
        "    X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n",
        "    y_tensor = torch.LongTensor(y)\n",
        "\n",
        "    # Compute saliency maps for images in X\n",
        "    saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n",
        "    # Convert the saliency map from Torch Tensor to numpy array and show images\n",
        "    # and saliency maps together.\n",
        "    saliency = saliency.numpy()\n",
        "    N = X.shape[0]\n",
        "    for i in range(N):\n",
        "        plt.subplot(2, N, i + 1)\n",
        "        plt.imshow(X[i])\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y[i]])\n",
        "        plt.subplot(2, N, N + i + 1)\n",
        "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
        "        plt.axis('off')\n",
        "        plt.gcf().set_size_inches(12, 5)\n",
        "    plt.show()\n",
        "\n",
        "show_saliency_maps(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya4LI0JJo7hf",
        "colab_type": "text"
      },
      "source": [
        "## Fooling classifiers (20 points)\n",
        "\n",
        "We can also use image gradients to generate \"fooling images\". Given an image and a target class, we can perform gradient ascent over the image to maximize the target class, stopping when the network classifies the image as the target class. Implement the following function to generate fooling images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh09QEQCo7hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_fooling_image(X, target_y, model):\n",
        "    \"\"\"\n",
        "    Generate a fooling image that is close to X, but that the model classifies\n",
        "    as target_y.\n",
        "\n",
        "    Inputs:\n",
        "    - X: Input image; Tensor of shape (1, 3, 224, 224)\n",
        "    - target_y: An integer in the range [0, 1000)\n",
        "    - model: A pretrained CNN\n",
        "\n",
        "    Returns:\n",
        "    - X_fooling: An image that is close to X, but that is classifed as target_y\n",
        "    by the model.\n",
        "    \"\"\"\n",
        "    # Initialize our fooling image to the input image, and make it require gradient\n",
        "    X_fooling = X.clone()\n",
        "    X_fooling = X_fooling.requires_grad_()\n",
        "    \n",
        "    learning_rate = 1\n",
        "    ##############################################################################\n",
        "    # TODO: Generate a fooling image X_fooling that the model will classify as   #\n",
        "    # the class target_y. You should perform gradient ascent on the score of the #\n",
        "    # target class, stopping when the model is fooled.                           #\n",
        "    # When computing an update step, first normalize the gradient:               #\n",
        "    #   dX = learning_rate * g / ||g||_2                                         #\n",
        "    #                                                                            #\n",
        "    # You should write a training loop.                                          #\n",
        "    #                                                                            #\n",
        "    # HINT: For most examples, you should be able to generate a fooling image    #\n",
        "    # in fewer than 100 iterations of gradient ascent.                           #\n",
        "    # You can print your progress over iterations to check your algorithm.       #\n",
        "    ##############################################################################\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                             END OF YOUR CODE                               #\n",
        "    ##############################################################################\n",
        "    return X_fooling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOzmwZ_ko7hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 2\n",
        "target_y = 76 # Tarantula\n",
        "# target_y = 78 # Tick\n",
        "# target_y = 187 # Yorkshire Terrier\n",
        "# target_y = 683 # Oboe\n",
        "# target_y = 366 # Gorilla\n",
        "# target_y = 604 # Hourglass\n",
        "\n",
        "X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n",
        "X_fooling = make_fooling_image(X_tensor[idx:idx+1], target_y, model)\n",
        "\n",
        "scores = model(X_fooling)\n",
        "assert target_y == scores.data.max(1)[1][0].item(), 'The model is not fooled!'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KfRLc5ao7hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_fooling_np = deprocess(X_fooling.clone())\n",
        "X_fooling_np = np.asarray(X_fooling_np).astype(np.uint8)\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(X[idx])\n",
        "plt.title(class_names[y[idx]])\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(X_fooling_np)\n",
        "plt.title(class_names[target_y])\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "X_pre = preprocess(Image.fromarray(X[idx]))\n",
        "diff = np.asarray(deprocess(X_fooling - X_pre, should_rescale=False))\n",
        "plt.imshow(diff)\n",
        "plt.title('Difference')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "diff = np.asarray(deprocess(10 * (X_fooling - X_pre), should_rescale=False))\n",
        "plt.imshow(diff)\n",
        "plt.title('Magnified difference (10x)')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.gcf().set_size_inches(12, 5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhybUoDQo7hp",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Recurrent Neural Networks (60 points)\n",
        "\n",
        "This part will be composed out of two steps:\n",
        "\n",
        "1. Understanding and implementing the vanilla RNN cell. As you learned in class, the RNN has a certain structure that allows it to accept the previous hidden state the current input, and output an hidden state and an output vector. The RNN cell uses the same weights for all time steps, much like convolution uses the same weights for all the batches in the image. Even though you already are familiar with PyTorch, implementing the RNN you make sure you understand how this pivotal architecture works.\n",
        "2. Using PyTorch to create a simple text generator. You will create a neural network using RNN cells and train it on your favorite text. Next, your network will generate text that \"feels\" like the text it was trained on. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r45JashQo7hq",
        "colab_type": "text"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAdomWH4o7hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
        "\n",
        "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
        "    \"\"\"\n",
        "    Evaluate a numeric gradient for a function that accepts a numpy\n",
        "    array and returns a numpy array.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h\n",
        "        pos = f(x).copy()\n",
        "        x[ix] = oldval - h\n",
        "        neg = f(x).copy()\n",
        "        x[ix] = oldval\n",
        "\n",
        "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
        "        it.iternext()\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E71Z9QJEo7hu",
        "colab_type": "text"
      },
      "source": [
        "# Vanilla RNN: step forward (7.5 points)\n",
        "\n",
        "First implement the function `rnn_step_forward` which implements the forward pass for a single timestep of a vanilla recurrent neural network. After doing so run the following to check your implementation. You should see errors less than 1e-8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--LNoTDyo7hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_step_forward(x, prev_h, Wx, Wh, b):\n",
        "    \"\"\"\n",
        "    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh\n",
        "    activation function.\n",
        "\n",
        "    The input data has dimension D, the hidden state has dimension H, and we use\n",
        "    a minibatch size of N.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data for this timestep, of shape (N, D).\n",
        "    - prev_h: Hidden state from previous timestep, of shape (N, H)\n",
        "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
        "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
        "    - b: Biases of shape (H,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - next_h: Next hidden state, of shape (N, H)\n",
        "    - cache: Tuple of values needed for the backward pass.\n",
        "    \"\"\"\n",
        "    next_h, cache = None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement a single forward step for the vanilla RNN. Store the next  #\n",
        "    # hidden state and any values you need for the backward pass in the next_h   #\n",
        "    # and cache variables respectively.                                          #\n",
        "    ##############################################################################\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return next_h, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPDTY-KEo7hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N, D, H = 3, 10, 4\n",
        "\n",
        "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
        "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
        "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
        "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
        "b = np.linspace(-0.2, 0.4, num=H)\n",
        "\n",
        "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
        "expected_next_h = np.asarray([\n",
        "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
        "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
        "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
        "\n",
        "print('next_h error: ', rel_error(expected_next_h, next_h))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBbmkR8to7h2",
        "colab_type": "text"
      },
      "source": [
        "# Vanilla RNN: step backward (7.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_gf4uxVo7h4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_step_backward(dnext_h, cache):\n",
        "    \"\"\"\n",
        "    Backward pass for a single timestep of a vanilla RNN.\n",
        "\n",
        "    Inputs:\n",
        "    - dnext_h: Gradient of loss with respect to next hidden state\n",
        "    - cache: Cache object from the forward pass\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradients of input data, of shape (N, D)\n",
        "    - dprev_h: Gradients of previous hidden state, of shape (N, H)\n",
        "    - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n",
        "    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
        "    - db: Gradients of bias vector, of shape (H,)\n",
        "    \"\"\"\n",
        "    dx, dprev_h, dWx, dWh, db = None, None, None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the backward pass for a single step of a vanilla RNN.      #\n",
        "    #                                                                            #\n",
        "    # HINT: For the tanh function, you can compute the local derivative in terms #\n",
        "    # of the output value from tanh.                                             #\n",
        "    ##############################################################################\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return dx, dprev_h, dWx, dWh, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWMz5MYZo7h-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1337)\n",
        "N, D, H = 4, 5, 6\n",
        "x = np.random.randn(N, D)\n",
        "h = np.random.randn(N, H)\n",
        "Wx = np.random.randn(D, H)\n",
        "Wh = np.random.randn(H, H)\n",
        "b = np.random.randn(H)\n",
        "\n",
        "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
        "\n",
        "dnext_h = np.random.randn(*out.shape)\n",
        "\n",
        "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n",
        "dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n",
        "dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n",
        "dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n",
        "db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n",
        "\n",
        "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
        "\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
        "print('dWx error: ', rel_error(dWx_num, dWx))\n",
        "print('dWh error: ', rel_error(dWh_num, dWh))\n",
        "print('db error: ', rel_error(db_num, db))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmzbfg3Mo7iC",
        "colab_type": "text"
      },
      "source": [
        "# Vanilla RNN: forward (7.5 points)\n",
        "Now that you have implemented the forward and backward passes for a single timestep of a vanilla RNN, you will combine these pieces to implement a RNN that process an entire sequence of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3eaojMDo7iD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_forward(x, h0, Wx, Wh, b):\n",
        "    \"\"\"\n",
        "    Run a vanilla RNN forward on an entire sequence of data. We assume an input\n",
        "    sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
        "    size of H, and we work over a minibatch containing N sequences. After running\n",
        "    the RNN forward, we return the hidden states for all timesteps.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data for the entire timeseries, of shape (N, T, D).\n",
        "    - h0: Initial hidden state, of shape (N, H)\n",
        "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
        "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
        "    - b: Biases of shape (H,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - h: Hidden states for the entire timeseries, of shape (N, T, H).\n",
        "    - cache: Values needed in the backward pass\n",
        "    \"\"\"\n",
        "    h, cache = None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for a vanilla RNN running on a sequence of    #\n",
        "    # input data. You should use the rnn_step_forward function that you defined  #\n",
        "    # above. You can use a for loop to help compute the forward pass.            #\n",
        "    ##############################################################################\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return h, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmFwaUtCo7iK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N, T, D, H = 2, 3, 4, 5\n",
        "\n",
        "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
        "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
        "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
        "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
        "b = np.linspace(-0.7, 0.1, num=H)\n",
        "\n",
        "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
        "expected_h = np.asarray([\n",
        "  [\n",
        "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
        "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
        "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
        "  ],\n",
        "  [\n",
        "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
        "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
        "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
        "print('h error: ', rel_error(expected_h, h))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zlf6eJ0o7iS",
        "colab_type": "text"
      },
      "source": [
        "# Vanilla RNN: backward (7.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDimImbbo7iV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_backward(dh, cache):\n",
        "    \"\"\"\n",
        "    Compute the backward pass for a vanilla RNN over an entire sequence of data.\n",
        "\n",
        "    Inputs:\n",
        "    - dh: Upstream gradients of all hidden states, of shape (N, T, H)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient of inputs, of shape (N, T, D)\n",
        "    - dh0: Gradient of initial hidden state, of shape (N, H)\n",
        "    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
        "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
        "    - db: Gradient of biases, of shape (H,)\n",
        "    \"\"\"\n",
        "    dx, dh0, dWx, dWh, db = None, None, None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the backward pass for a vanilla RNN running an entire      #\n",
        "    # sequence of data. You should use the rnn_step_backward function that you   #\n",
        "    # defined above. You can use a for loop to help compute the backward pass.   #\n",
        "    ##############################################################################\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return dx, dh0, dWx, dWh, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuXn5Y3uo7iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1337)\n",
        "\n",
        "N, D, T, H = 2, 3, 10, 5\n",
        "\n",
        "x = np.random.randn(N, T, D)\n",
        "h0 = np.random.randn(N, H)\n",
        "Wx = np.random.randn(D, H)\n",
        "Wh = np.random.randn(H, H)\n",
        "b = np.random.randn(H)\n",
        "\n",
        "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
        "\n",
        "dout = np.random.randn(*out.shape)\n",
        "\n",
        "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
        "\n",
        "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
        "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
        "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
        "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
        "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
        "\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
        "print('dWx error: ', rel_error(dWx_num, dWx))\n",
        "print('dWh error: ', rel_error(dWh_num, dWh))\n",
        "print('db error: ', rel_error(db_num, db))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwHl43YLo7ic",
        "colab_type": "text"
      },
      "source": [
        "# Generating text using RNNs (30 points)\n",
        "\n",
        "Using PyTorch, create a network that is capable of generating text, similar to the text it has seen during training. In order to tackle this problem, first read the [following blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy for some creative uses of this network and his implemention of a [char-based RNN in pure numpy](https://gist.github.com/karpathy/d4dee566867f8291f086). \n",
        "\n",
        "While implementing your own network (in PyTorch), make sure to consider the following:\n",
        "\n",
        "1. Get a large enough text file, with proper encoding. A solid place to start should the creations of [Shakespere](https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt). You are encourged to try different datasets with amusing outcomes.\n",
        "2. Encode the text and map each character to an integer. One-hot encoding might also be a good idea.\n",
        "3. You might be temped to use a dataloader, however defining your own method to obtain training batches might be easier.\n",
        "4. Define your model. The following guide will help you understand how to use RNNs in PyTorch: [RNN text classification](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html). You model should be relatively simple.\n",
        "5. Train your model. Training a proper model might take a while, so you are encoraged to use [Colab](https://colab.research.google.com/).\n",
        "6. Create a function that takes the learned network and predicts a single character. This function should take a hidden state and an input character, and output the next hidden state and the predicted character.\n",
        "7. Finally, create a sampling function that takes the network, the required length of text to generate and an initial input and generate some text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENxsnwiho7ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data/shakespeare.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUqU9raco7ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Your code here. Add as many cells as you need ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtYMBjngo7il",
        "colab_type": "text"
      },
      "source": [
        "## Class visualization (bonus 5 points)\n",
        "By starting with a random noise image and performing gradient ascent on a target class, we can generate an image that the network will recognize as the target class. Concretely, let $I$ be an image and let $y$ be a target class. Let $s_y(I)$ be the score that a convolutional network assigns to the image $I$ for class $y$; note that these are raw unnormalized scores, not class probabilities. We wish to generate an image $I^*$ that achieves a high score for the class $y$ by solving the problem\n",
        "\n",
        "$$\n",
        "I^* = \\arg\\max_I (s_y(I) - R(I))\n",
        "$$\n",
        "where $R$ is a (possibly implicit) regularizer (note the sign of $R(I)$ in the argmax: we want to minimize this regularization term). We can solve this optimization problem using gradient ascent, computing gradients with respect to the generated image. We will use (explicit) L2 regularization of the form\n",
        "\n",
        "$$\n",
        "R(I) = \\lambda \\|I\\|_2^2\n",
        "$$\n",
        "and implicit by periodically blurring the generated image. We can solve this problem using gradient ascent on the generated image.\n",
        "\n",
        "In the cell below, complete the implementation of the create_class_visualization function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hju6Gvufo7im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jitter(X, ox, oy):\n",
        "    \"\"\"\n",
        "    Helper function to randomly jitter an image.\n",
        "    \n",
        "    Inputs\n",
        "    - X: PyTorch Tensor of shape (N, C, H, W)\n",
        "    - ox, oy: Integers giving number of pixels to jitter along W and H axes\n",
        "    \n",
        "    Returns: A new PyTorch Tensor of shape (N, C, H, W)\n",
        "    \"\"\"\n",
        "    if ox != 0:\n",
        "        left = X[:, :, :, :-ox]\n",
        "        right = X[:, :, :, -ox:]\n",
        "        X = torch.cat([right, left], dim=3)\n",
        "    if oy != 0:\n",
        "        top = X[:, :, :-oy]\n",
        "        bottom = X[:, :, -oy:]\n",
        "        X = torch.cat([bottom, top], dim=2)\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogWsY8clo7ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_class_visualization(target_y, model, dtype, **kwargs):\n",
        "    \"\"\"\n",
        "    Generate an image to maximize the score of target_y under a pretrained model.\n",
        "    \n",
        "    Inputs:\n",
        "    - target_y: Integer in the range [0, 1000) giving the index of the class\n",
        "    - model: A pretrained CNN that will be used to generate the image\n",
        "    - dtype: Torch datatype to use for computations\n",
        "    \n",
        "    Keyword arguments:\n",
        "    - l2_reg: Strength of L2 regularization on the image\n",
        "    - learning_rate: How big of a step to take\n",
        "    - num_iterations: How many iterations to use\n",
        "    - blur_every: How often to blur the image as an implicit regularizer\n",
        "    - max_jitter: How much to jitter the image as an implicit regularizer\n",
        "    - show_every: How often to show the intermediate result\n",
        "    \"\"\"\n",
        "    model.type(dtype)\n",
        "    l2_reg = kwargs.pop('l2_reg', 1e-3)\n",
        "    learning_rate = kwargs.pop('learning_rate', 25)\n",
        "    num_iterations = kwargs.pop('num_iterations', 500)\n",
        "    blur_every = kwargs.pop('blur_every', 10)\n",
        "    max_jitter = kwargs.pop('max_jitter', 16)\n",
        "    show_every = kwargs.pop('show_every', 100)\n",
        "\n",
        "    # Randomly initialize the image as a PyTorch Tensor, and make it requires gradient.\n",
        "    img = torch.randn(1, 3, 224, 224).mul_(1.0).type(dtype).requires_grad_()\n",
        "\n",
        "    for t in range(num_iterations):\n",
        "        # Randomly jitter the image a bit; this gives slightly nicer results\n",
        "        ox, oy = random.randint(0, max_jitter), random.randint(0, max_jitter)\n",
        "        img.data.copy_(jitter(img.data, ox, oy))\n",
        "\n",
        "        ########################################################################\n",
        "        # TODO: Use the model to compute the gradient of the score for the     #\n",
        "        # class target_y with respect to the pixels of the image, and make a   #\n",
        "        # gradient step on the image using the learning rate. Don't forget the #\n",
        "        # L2 regularization term!                                              #\n",
        "        # Be very careful about the signs of elements in your code.            #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        "        \n",
        "        # Undo the random jitter\n",
        "        img.data.copy_(jitter(img.data, -ox, -oy))\n",
        "\n",
        "        # As regularizer, clamp and periodically blur the image\n",
        "        for c in range(3):\n",
        "            lo = float(-SQUEEZENET_MEAN[c] / SQUEEZENET_STD[c])\n",
        "            hi = float((1.0 - SQUEEZENET_MEAN[c]) / SQUEEZENET_STD[c])\n",
        "            img.data[:, c].clamp_(min=lo, max=hi)\n",
        "        if t % blur_every == 0:\n",
        "            blur_image(img.data, sigma=0.5)\n",
        "        \n",
        "        # Periodically show the image\n",
        "        if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1:\n",
        "            print(img.data.clone().cpu())\n",
        "            plt.imshow(deprocess(img.data.clone().cpu()))\n",
        "            class_name = class_names[target_y]\n",
        "            plt.title('%s\\nIteration %d / %d' % (class_name, t + 1, num_iterations))\n",
        "            plt.gcf().set_size_inches(4, 4)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "    return deprocess(img.data.cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTrGPX1jo7iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtype = torch.FloatTensor\n",
        "# dtype = torch.cuda.FloatTensor # Uncomment this to use GPU\n",
        "model.type(dtype)\n",
        "\n",
        "# target_y = 76 # Tarantula\n",
        "# target_y = 78 # Tick\n",
        "# target_y = 187 # Yorkshire Terrier\n",
        "# target_y = 683 # Oboe\n",
        "# target_y = 366 # Gorilla\n",
        "# target_y = 604 # Hourglass\n",
        "\n",
        "out = create_class_visualization(target_y, model, dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}